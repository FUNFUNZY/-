L1正则化（L1范数）：权值向量w中各个元素的绝对值之和
L2正则化（L2范数）：权值向量w中各个元素的平方和

问：L1比L2更容易让参数变得稀疏？why？
答：L1有更多突出的角，而这些角上很多权值w取值为0，另外角更突出，所以更加容易与经验风险最小化部分产生交叉求得解。

# loss = tf.reduce_mean(tf.square(y - y_pred))

# 结构风险=经验风险+正则化，经验风险使用交叉熵，正则化使用L2。
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = finalOutput))\
#        + tf.contrib.layers.l2_regularizer(0.05)(W1) + tf.contrib.layers.l2_regularizer(0.05)(W2) +  tf.contrib.layers.l2_regularizer(0.05)(W3)  


原文出处：https://www.jianshu.com/p/1620343c9f3c


L2与L1的区别在于，L1正则是拉普拉斯先验，而L2正则则是高斯先验。它们都是服从均值为0，协方差为1λ1λ。当λ=0λ=0时，即没有先验）没有正则项，
则相当于先验分布具有无穷大的协方差，那么这个先验约束则会非常弱，模型为了拟合所有的训练集数据， 参数ww可以变得任意大从而使得模型不稳定，
即方差大而偏差小。λλ越大，标明先验分布协方差越小，偏差越大，模型越稳定。即，加入正则项是在偏差bias与方差variance之间做平衡tradeoff（来自知乎）。

原文链接：https://blog.csdn.net/heyongluoyao8/java/article/details/49429629
