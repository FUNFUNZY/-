先说熵与极大似然：
似然函数实际上是判断估计模型与经验分布的相似度，最大似然估计就是让估计模型尽可能的去接近我们得到样本产生的经验分布。
而熵的定义中只有概率模型本身，它表示的是该模型本身的每个取值之间的不确定性大小，或者说是该模型本身的混乱程度。

熵与极大似然的一个重要区别在于熵是描述模型本身混乱度（信息量）的一个度量，而极大似然是描述估计模型和经验分布的相似度的一个度量。


再说交叉熵与熵：
熵描述模型内部的信息量，而交叉熵则描述两个模型之间的关系。
最小化交叉熵的本质就是对数似然函数的最大化。

参考相关数学推导

个人总结：损失函数最优化是求目标函数最小值，用极大似然的思想求极小值就是求负的极大似然；
而交叉熵，数学基础实际上就是负的极大似然函数，即最小化交叉熵就是最大化似然





机器学习中常见的三个损失函数：mean square loss, binary cross entropy loss, categories cross entropy，其背后的原理都指向了极大似然估计。
可见损失函数的定义并不是简单的'衡量预测值与真实值的距离'，其背后有着严谨的数学在为其可靠性做担保。


http://www.360doc.com/content/20/0512/22/32196507_911857909.shtml
https://www.cnblogs.com/breezezz/p/11277131.html#31-%E8%81%94%E7%B3%BB
