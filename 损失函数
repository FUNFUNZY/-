####点到即止，主要是理清思路做个笔记，具体的参考原文再按点深入####
####文章为摘抄整理####

在机器学习中，损失函数（loss function）是用来估量模型的预测值f(x)与真实值Y的不一致程度，
损失函数越小，一般就代表模型的鲁棒性越好，正是损失函数指导了模型的学习。---------最优化问题



一、回归损失
（1）平方损失函数、均方误差、L2损失（最小二乘法, Ordinary Least Squares ）
最小二乘的基本原则是：最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小。
目的就是最小化目标函数值，即最小化残差的平方和（residual sum of squares，RSS）。

均方误差（MSE）度量的是预测值和实际观测值间差的平方的均值。它只考虑误差的平均大小，不考虑其方向。
但由于经过平方，与真实值偏离较多的预测值会比偏离较少的预测值受到更为严重的惩罚。再加上 MSE 的数学特性很好，这使得计算梯度变得更容易。

（2）平均绝对误差/L1 损失
均绝对误差（MAE）度量的是预测值和实际观测值之间绝对差之和的平均值。和 MSE 一样，这种度量方法也是在不考虑方向的情况下衡量误差大小。
但和 MSE 的不同之处在于，MAE 需要像线性规划这样更复杂的工具来计算梯度。此外，MAE 对异常值更加稳健，因为它不使用平方。

补充：L1 loss与L2 loss的改进
原始的L1 loss和L2 loss都有缺陷，比如L1 loss的最大问题是梯度不平滑，而L2 loss的最大问题是容易梯度爆炸。
为了增强L2 loss对噪声(离群点)的鲁棒性，研究者提出了Huber loss，Huber对于离群点非常的有效，它同时结合了L1与L2的优点，
不过多出来了一个delta参数需要进行训练。

（3）平均偏差误差（mean bias error）
与其它损失函数相比，这个函数在机器学习领域没有那么常见。它与 MAE 相似，唯一的区别是这个函数没有用绝对值。用这个函数需要注意的一点是，
正负误差可以互相抵消。尽管在实际应用中没那么准确，但它可以确定模型存在正偏差还是负偏差。

二、分类损失
（1）Hinge Loss/多分类 SVM 损失

在一定的安全间隔内（通常是 1），正确类别的分数应高于所有错误类别的分数之和。
因此 hinge loss 常用于最大间隔分类（maximum-margin classification），最常用的是支持向量机。
尽管不可微，但它是一个凸函数，因此可以轻而易举地使用机器学习领域中常用的凸优化器。
机器学习算法中，hinge损失函数和SVM是息息相关的

在libsvm中一共有4中核函数可以选择，对应的是-t参数分别是：
0-线性核；
1-多项式核；
2-RBF核；
3-sigmoid核。

（2）交叉熵损失/负对数似然/LogLoss对数损失函数：（逻辑回归）

这是分类问题中最常见的设置。随着预测概率偏离实际标签，交叉熵损失会逐渐增加。
注意，当实际标签为 1(y(i)=1) 时，函数的后半部分消失，而当实际标签是为 0(y(i=0)) 时，函数的前半部分消失。
简言之，我们只是把对真实值类别的实际预测概率的对数相乘。还有重要的一点是，交叉熵损失会重重惩罚那些置信度高但是错误的预测值。


逻辑回归的损失函数不是平方损失。平方损失函数(最小二乘法)可以通过线性回归在假设样本是高斯分布的条件下推导得到，
在逻辑回归的推导中，它假设样本服从伯努利分布（0-1分布），然后求得满足该分布的似然函数，接着取对数求极值等等。
而逻辑回归并没有求似然函数的极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：
最小化负的似然函数（即max F(y, f(x)) —> min -F(y, f(x)))。从损失函数的视角来看，它就成了log损失函数了。

损失函数L(Y, P(Y|X))表达的是样本X在分类Y的情况下，使概率P(Y|X)达到最大值（换言之，就是利用已知的样本分布，
找到最有可能（即最大概率）导致
这种分布的参数值；或者说什么样的参数才能使我们观测到目前这组数据的概率最大）。

softmax使用的即为交叉熵损失函数，
binary_cossentropy为二分类交叉熵损失，
categorical_crossentropy为多分类交叉熵损失，当使用多分类交叉熵损失函数时，标签应该为多分类模式，即使用one-hot编码的向量。one-hot=True


三、指数损失函数（Adaboost）
前向分步加法算法
Adaboost的目标式子就是指数损失



四、其它损失函数
0-1损失函数
绝对值损失函数


五、Keras / TensorFlow 中常用 Cost Function 总结

mean_squared_error或mse
mean_absolute_error或mae
mean_absolute_percentage_error或mape
mean_squared_logarithmic_error或msle
squared_hinge
hinge
categorical_hinge
binary_crossentropy（亦称作对数损失，logloss）
logcosh
categorical_crossentropy：亦称作多类的对数损失，注意使用该目标函数时，需要将标签转化为形如(nb_samples, nb_classes)的二值序列
sparse_categorical_crossentrop：如上，但接受稀疏标签。注意，使用该函数时仍然需要你的标签与输出值的维度相同，
                                你可能需要在标签数据上增加一个维度：np.expand_dims(y,-1)
kullback_leibler_divergence:从预测值概率分布Q到真值概率分布P的信息增益,用以度量两个分布的差异.
poisson：即(predictions - targets * log(predictions))的均值
cosine_proximity：即预测值与真实标签的余弦距离平均值的相反数

需要记住的是：参数越多，模型越复杂，而越复杂的模型越容易过拟合。
过拟合就是说模型在训练数据上的效果远远好于在测试集上的性能。
此时可以考虑正则化，通过设置正则项前面的hyper parameter，来权衡损失函数和正则项，减小参数规模，达到模型简化的目的，
从而使模型具有更好的泛化能力。


图文参考：
原文链接：https://www.cnblogs.com/guoyaohua/p/9217206.html
          https://www.jiqizhixin.com/articles/091202
