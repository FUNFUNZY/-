参考至文章链接：https://www.cnblogs.com/maybe2030/p/9163479.html

R采用了最小化交叉熵或者最大化似然估计函数来作为Cost Function，那为什么我们不用更加简单熟悉的最小化平方误差函数（MSE）呢？

我个人理解主要有三个原因：
MSE的假设是高斯分布，交叉熵的假设是伯努利分布，而逻辑回归采用的就是伯努利分布；
MSE会导致代价函数J(θ)非凸，这会存在很多局部最优解，而我们更想要代价函数是凸函数；
MSE相对于交叉熵而言会加重梯度弥散。

其实可以从另外一个角度理解为什么交叉熵函数相对MSE不易导致梯度弥散：当训练结果接近真实值时会因为梯度算子极小，使得模型的收敛速度变得非常的缓慢。
而由于交叉熵损失函数为对数函数，在接近上边界的时候，其仍然可以保持在高梯度状态，因此模型的收敛速度不会受损失函数的影响。


代价函数为什么要为凸函数？
为什么MSE会更易导致梯度弥散？

Cost Function和Loss Function的区别
Cost Function：指基于参数w和b，在所有训练样本上的总成本；
Loss Function：指单个训练样本的损失函数。


参考链接：https://www.cnblogs.com/maybe2030/p/6336896.html

梯度消失：

目前深度学习面临的一个问题就是在网络训练的过程中存在梯度消失问题（vanishing gradient problem），或者更广义地来讲就是不稳定梯度问题。
激增的梯度问题（exploding gradient problem），这也没有比消失的梯度问题更好处理。更加一般地说，在深度神经网络中的梯度是不稳定的，在前面的层中或会消失，
或会激增，这种不稳定性才是深度神经网络中基于梯度学习的根本原因。

什么导致了梯度消失？
梯度消失的本质原因是：ωjσ′(zj)<1/4的约束。
不稳定的梯度问题：根本的问题其实并非是消失的梯度问题或者激增的梯度问题，而是在前面的层上的梯度是来自后面的层上项的乘积。
唯一让所有层都接近相同的学习速度的方式是所有这些项的乘积都能得到一种平衡。



