参考至文章链接：https://www.cnblogs.com/maybe2030/p/9163479.html

R采用了最小化交叉熵或者最大化似然估计函数来作为Cost Function，那为什么我们不用更加简单熟悉的最小化平方误差函数（MSE）呢？

我个人理解主要有三个原因：
MSE的假设是高斯分布，交叉熵的假设是伯努利分布，而逻辑回归采用的就是伯努利分布；
MSE会导致代价函数J(θ)非凸，这会存在很多局部最优解，而我们更想要代价函数是凸函数；
MSE相对于交叉熵而言会加重梯度弥散。

其实可以从另外一个角度理解为什么交叉熵函数相对MSE不易导致梯度弥散：当训练结果接近真实值时会因为梯度算子极小，使得模型的收敛速度变得非常的缓慢。
而由于交叉熵损失函数为对数函数，在接近上边界的时候，其仍然可以保持在高梯度状态，因此模型的收敛速度不会受损失函数的影响。


代价函数为什么要为凸函数？


为什么MSE会更易导致梯度弥散？






Cost Function和Loss Function的区别
Cost Function：指基于参数w和b，在所有训练样本上的总成本；
Loss Function：指单个训练样本的损失函数。
