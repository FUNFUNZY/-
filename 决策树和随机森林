转载：https://www.cnblogs.com/sxron/p/5471078.html

部分笔记

1、决策树分类原理
决策树分为分类树和回归树两种，分类树对离散变量做决策树，回归树对连续变量做决策树。
构造决策树的过程本质上就是根据数据特征将数据集分类的递归过程，我们需要解决的第一个问题就是，当前数据集上哪个特征在划分数据分类时起决定性作用。

2、决策树的学习过程
一棵决策树的生成过程主要分为以下3个部分:
    特征选择：特征选择是指从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着很多不同量化评估标准标准，从而衍生出不同的决策树算法。
    决策树生成： 根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则停止决策树停止生长。 树结构来说，递归结构是最容易理解的方式。
    剪枝：决策树容易过拟合，一般来需要剪枝，缩小树结构规模、缓解过拟合。剪枝技术有预剪枝和后剪枝两种。

3、基于信息论的三种决策树算法
划分数据集的最大原则是：使无序的数据变的有序
基于信息论的决策树算法有ID3、CART和C4.5等算法，其中C4.5和CART两种算法从ID3算法中衍生而来。
CART和C4.5支持数据特征为连续分布时的处理，主要通过使用二元切分来处理连续型变量，即求一个特定的值-分裂值：特征值大于分裂值就走左子树，或者就走右子树。
这个分裂值的选取的原则是使得划分后的子树中的“混乱程度”降低，具体到C4.5和CART算法则有不同的定义方式。

### ID3算法中根据信息论的信息增益评估和选择特征，每次选择信息增益最大的特征做判断模块。ID3算法可用于划分标称型数据集。ID3不能处理连续分布的数据特征
### C4.5算法用信息增益率来选择属性，能够完成对连续属性的离散化处理；能够对不完整数据进行处理。C4.5只适合于能够驻留于内存的数据集。
              （C4.5既可以处理离散型属性，也可以处理连续性属性）
### CART算法的全称是Classification And Regression Tree，采用的是Gini指数（选Gini指数最小的特征s）作为分裂标准,同时它也是包含后剪枝操作。

4、决策树优缺点

决策树适用于数值型和标称型（离散型数据，变量的结果只在有限目标集中取值），能够读取数据集合，提取一些列数据中蕴含的规则。在分类问题中使用决策树模型有很多的优点，
决策树计算复杂度不高、便于使用、而且高效，决策树可处理具有不相关特征的数据、可很容易地构造出易于理解的规则，而规则通常易于解释和理解。

决策树模型也有一些缺点，比如处理缺失数据时的困难、过度拟合以及忽略数据集中属性之间的相关性等。


（二）ID3算法的数学原理
（1）信息熵
　　信息熵：在概率论中，信息熵给了我们一种度量不确定性的方式，是用来衡量随机变量不确定性的，熵就是信息的期望值。
 
 (2)条件熵

　　假设有随机变量(X,Y)，其联合概率分布为:P(X=xi,Y=yi)=pij,i=1,2,⋯,n;j=1,2,⋯,m
　　则条件熵(H(Y∣X))表示在已知随机变量X的条件下随机变量Y的不确定性



