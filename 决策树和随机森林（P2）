https://www.jianshu.com/p/1f0b7705914d

随机森林是一种由决策树构成的集成算法. 随机森林属于集成学习中的 Bagging. 用随机的方式建立一个森林，森林里面有很多的决策树组成，
随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，
看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。


随机森林的构造过程：
1.假如有 n 个样本，有放回的随机选择 n 个样本(每次随机选择一个样本，然后放回继续选择)。这样就选择好了 n 个样本，用来训练一个决策树，
   作为决策树根节点处的样本。
2.当每个样本有 m 个属性是，在决策树的每个节点需要分裂时，随机从这 m个属性中选取初 k 个属性，满足条件 k<<m。然后从这 k 个属性种采用某种策略
   (ID3、C4.5、CART)来选择 1 个属性作为该节点的分裂属性。
3.决策树形成过程中每个节点都要按照 2 来分裂(如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到叶子节点，无须继续分裂了)。
    一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。
4.按照步骤 1～3 建立大量的决策树，这样就构成了随机森林。


随机森林中的“随机”就是指的这里的两个随机性。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。

随机森林分类效果（错误率）与两个因素有关：
１.森林中任意两棵树的相关性：相关性越大，错误率越大；
２.森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。
　　减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），
    这也是随机森林唯一的一个参数。

构建随机森林的关键问题就是如何选择最优的m，要解决这个问题主要依据计算袋外错误率oob error（out-of-bag error）。

https://www.cnblogs.com/maybe2030/p/4585705.html












