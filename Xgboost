[Installation Guide] from: https://xgboost.readthedocs.io/en/latest/build.html

XGBoost是boosting算法的其中一种。Boosting算法的思想是将许多弱分类器集成在一起形成一个强分类器。
XGBoost是一种梯度提升树模型，它是将许多树模型集成在一起，形成一个很强的分类器，所用到的树模型则是CART回归树模型。

XGBoost算法思想就是不断地添加树，不断地进行特征分裂来完成一棵树的构建。每次添加一个树，实际上是学习一个新函数，去拟合上次预测的残差。我们训练完成时会得到k棵树 。
当我们要预测一个样本的分数时，根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，
每个叶子节点就对应一个分数，最后只需要将每棵树对应的分数加起来就是该样本的预测值。

剪枝split a leaf into two leaves

Model IO：
saving/loading model: booster.save_model('model.json')
保存模型语法：bst.save_model('model_file_name.json')
when one calls booster.save_model , XGBoost saves the trees

Saving and Loading the internal parameters configuration：
API：
bst = xgboost.train(...)
config = bst.save_config()
print(config)

bst.load_config(config)

模型调参
参数链接：https://zhuanlan.zhihu.com/p/95304498

##  普通参数 [General Parameters]
1. booster [default: gbtree]
    a: 表示应用的弱学习器的类型, 推荐用默认参数
    b: 可选的有gbtree, dart, gblinear
        gblinear是线性模型，表现很差，接近一个LASSO
        dart是树模型的一种，思想是每次训练新树的时候，随机从前m轮的树中扔掉一些，来避免过拟合
        gbtree即是论文中主要讨论的树模型，推荐使用

2. silent [default: 0] [不推荐]
    a: 不推荐使用，推荐使用verbosity参数来代替，功能更强大

3. verbosity [default: 1]
    a: 训练过程中打印的日志等级，0 (silent), 1 (warning), 2 (info), 3 (debug)

4. nthread [default: 最大可用线程数][alias: n_jobs]
    a: 训练过程中的并行线程数
    b: 如果用的是sklearn的api，那么使用n_jobs来代替


## 每个分类器算法参数 [Booster Parameters]
1. eta [default: 0.3] [alias: learning_rate] [range: [0, 1]]
    a: 就是常说的学习速率，控制每一次学习的权重缩减，给后来的模型提供更多的学习空间

2. gamma [default: 0] [alias: min_split_loss] [range: [0, inf]]
    a: 叶子节点分裂时所需要的最小的损失减少量，这个值越大，叶子节点越难分裂，所以算法就越保守
    {正则化项控制叶子节点数量复杂度( gama*T )的系数}
    
3. max_depth [default: 6] [range: [0, inf]]
    a: 树的最大深度
    b: 这个值对结果的影响算是比较大的了，值越大，树的深度越深，模型的复杂度就越高，就越容易过拟合
    c: 注意如果这个值被设置的较大，会吃掉大量的内存
    d: 一般来说比价合适的取值区间为[3, 10]

4. min_child_weight [default: 1] [range: [0, inf]]
    a: 最小的叶子节点权重
    b: 在普通的GBM中，叶子节点样本没有权重的概念，其实就是等权重的，也就相当于叶子节点样本个数
    c: 越小越没有限制，容易过拟合，太高容易欠拟合
    
5. max_delta_step [default: 0] [range: [0, inf]]
    a: 适用于正负样本不均衡情况下，控制学习速率(类似eta)最大为某个值，不能超过这个阈值
    b: 首先我们有参数eta来控制学习速率，为了后面学习到更多，每一步在权重上乘上这个因子，降低速度
    c: 但是在正负样本不均衡的情况下eta不足够，因为此时由于二阶导接近于0的原因，权重会特别大
    d: 这个参数就是用来控制学习速率最大不能超过这个数值

6. sub_sample [default: 1] [range: (0, 1]]
    a: 样本抽样比例
    b: 在每次训练的随机选取sub_sample比例的样本来作为训练样本

7. colsample_by* [default: 1]
    a: 这里实际上有3个参数，借助了随机森林的特征抽样的思想，3个参数可以同时使用
    b: colsample_bytree   更常用，每棵树的特征抽样比例
    c: colsample_bylevel  每一层深度的树特征抽样比例
    d: colsample_bynode   每一个节点的特征抽样比例

8. lambda [default: 1] [alias: reg_lambda]
    a: 损失函数中的L2正则化项的系数，类似RidgeRegression，减轻过拟合

9. alpha [default: 0] [alias: reg_alpha]
    a: 损失函数中的L1正则化项的系数，类似LASSO，减轻过拟合

10. scale_pos_weight [default: 1]
    a: 在正负样本不均衡的情况下，此参数需要设置，通常为: sum(负样本) / sum(正样本)
    

samples源代码: https://github.com/dmlc/xgboost/tree/master/demo/guide-python
https://github.com/dmlc/xgboost/blob/master/demo/guide-python/sklearn_examples.py


