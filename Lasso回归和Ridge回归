(Least absolute shrinkage and selection operator)

Lasso方法是以缩小变量集（降阶）为思想的压缩估计方法。
它通过构造一个惩罚函数，可以将变量的系数进行压缩并使某些回归系数变为0，进而达到变量选择的目的。

过拟合的问题通常发生在变量（特征）过多的时候，这种情况下训练出的方程总是能很好的拟合训练数据，
也就是损失函数可能非常接近于 0 或者就为 0。 但是，这样的曲线会导致它无法泛化到新的数据样本中。

Lasso回归是在损失函数后，加L1正则化

岭回归是在损失函数后，加L2正则化

Lasso的复杂程度由λ来控制，λ越大对变量较多的线性模型的惩罚力度就越大，从而最终获得一个变量较少的模型。

lasso回归和ridge回归的区别在于约束项和等高线不同。

等高线和约束域的切点就是目标函数的最优解，Ridge方法对应的约束域是圆，其切点只会存在于圆周上，不会与坐标轴相切，
则在任一维度上的取值都不为0，因此没有稀疏；
对于Lasso方法,其约束域是正方形，会存在与坐标轴的切点，使得部分维度特征权重为0，因此很容易产生稀疏的结果。 

Lasso方法可以达到变量选择的效果，将不显著的变量系数压缩至0，而Ridge方法虽然也对原本的系数进行了一定程度的压缩，
但是任一系数都不会压缩至0，最终模型保留了所有的变量。


文章链接：https://blog.csdn.net/foneone/article/details/96576990
