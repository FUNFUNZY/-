神經網絡的訓練可以分為兩個步驟：一個是前向傳播，另外一個是反向傳播。

神經網絡就是通過不斷的前向傳播和反向傳播不斷調整神經網絡的權重，最終到達預設的疊代次數或者對樣本的學習已經到了比較好的程度後，就停止疊代，
那麼一個神經網絡就訓練好了。

前向傳播就是從輸入層開始（Layer1），經過一層層的Layer，不斷計算每一層的z和a，最後得到輸出y^的過程。

我們可以將神經網絡分解為許多個神經元，每個神經元接收上一層的輸入進行簡單的邏輯回歸操作，全部神經元從輸入層開始到輸出層依次進行邏輯回歸的過程我們
可以簡單的理解為神經網絡的前向傳播。

前向傳播是從神經網絡的輸入層開始，逐漸往輸出層進行前向傳播，上一層的神經元與本層的神經元有連接，那麼本層的神經元的激活等於上一層神經元對應的權值
進行加權和運算，最後通過一個非線性函數（激活函數）如ReLu，sigmoid等函數，最後得到的結果就是本層神經元的輸出。神經網絡逐層逐神經元通過該操作向前傳播，
最終得到輸出層的結果。



原文網址：https://kknews.cc/code/p9jkxlj.html


[Deep Learning] 神经网络基础：https://www.cnblogs.com/maybe2030/p/5597716.html（这篇文章写得太棒啦！！）

感知机模型可以由如下公式表示：y=f(wx+b)
我们日常生活中很多问题，甚至说大多数问题都不是线性可分问题，那我们要解决非线性可分问题该怎样处理呢？
既然单层感知机解决不了非线性问题，那我们就采用多层感知机事实上，感知机是一种判别式的线性分类模型，可以解决与、或、非这样的简单的线性可分（linearly separable）问题，
常将多层感知机这样的多层结构称之为是神经网络

误差逆传播算法（— BP学习算法——）
　所谓神经网络的训练或者是学习，其主要目的在于通过学习算法得到神经网络解决指定问题所需的参数，这里的参数包括各层神经元之间的连接权重以及偏置等。
通常是根据实际问题来构造出网络结构，参数的确定则需要神经网络通过训练样本和学习算法来迭代找到最优参数组。
  BP学习算法通常用在最为广泛使用的多层前馈神经网络中。
 
 　　BP算法的主要流程可以总结如下：
　　输入：训练集D=(xk,yk)mk=1; 学习率;
　　过程：
　　1. 在(0, 1)范围内随机初始化网络中所有连接权和阈值
　　2. repeat:
　　3.　　 for all (xk,yk)∈D do
　　4. 　　　　根据当前参数计算当前样本的输出;
　　5. 　　　　计算输出层神经元的梯度项；
　　6. 　　　　计算隐层神经元的梯度项；
　　7. 　　　　更新连接权与阈值
　　8. 　　end for
　　9. until 达到停止条件
　　输出：连接权与阈值确定的多层前馈神经网络
 
神经网络模型
Boltzmann机：标准的Boltzmann机是全连接的，也就是说各层内的神经元都是相互连接的，因此计算复杂度很高，而且难以用来解决实际问题
受限玻尔兹曼机（Restricted Boltzmann Mechine，简称RBM）：层内无连接，层间有连接
　       RBM常常用对比散度（Constrastive Divergence，简称CD）来进行训练。


一般的神经网络都是先指定好网络结构，训练的目的是利用训练样本来确定合适的连接权、阈值等参数。

递归神经网络（Recurrent Neural Networks，简称RNN）允许网络中出现环形结构，从而可以让一些神经元的输出反馈回来作为输入信号，
这样的结构与信息反馈过程，使得网络在t时刻的输出状态不仅与t时刻的输入有关，还与t−1时刻的网络状态有关，从而能处理与时间有关的动态变化。
RNN一般的训练算法采用推广的BP算法。值得一提的是，RNN在（t+1）时刻网络的结果O(t+1)是该时刻输入和所有历史共同作用的结果，这样就达到了
对时间序列建模的目的。

RNN在（t+1）时刻网络的结果O(t+1)是该时刻输入和所有历史共同作用的结果，这么讲其实也不是很准确，因为“梯度发散”同样也会发生在时间轴上，也就是说对于t时刻来说，它产生的梯度在时间轴上向历史传播几层之后就消失了，根本无法影响太遥远的过去。因此，“所有的历史”只是理想的情况。在实际中，这种影响也就只能维持若干个时间戳而已。换句话说，后面时间步的错误信号，往往并不能回到足够远的过去，像更早的时间步一样，去影响网络，这使它很难以学习远距离的影响。
为了解决上述时间轴上的梯度发散，机器学习领域发展出了长短时记忆单元（Long-Short Term Memory，简称LSTM），通过门的开关实现时间上的记忆功能，并防止梯度发散。其实除了学习历史信息，RNN和LSTM还可以被设计成为双向结构，即双向RNN、双向LSTM，同时利用历史和未来的信息。

深度学习
深度学习指的是深度神经网络模型，一般指网络层数在三层或者三层以上的神经网络结构。

 为了解决深层神经网络的训练问题，
 一种有效的手段是采取无监督逐层训练（unsupervised layer-wise training），其基本思想是每次训练一层隐节点，训练时将上一层
 隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，这被称之为“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”
 （fine-tunning）训练。------>“预训练+微调”
 　事实上，“预训练+微调”的训练方式可被视为是将大量参数分组，对每组先找到局部看起来较好的设置，然后再基于这些局部较优的结果联合起来进行全局寻优。
  这样就在利用了模型大量参数所提供的自由度的同时，有效地节省了训练开销。
 
另一种节省训练开销的做法是进行“权共享”（weight sharing），即让一组神经元使用相同的连接权，这个策略
在卷积神经网络（Convolutional Neural Networks，简称CNN）中发挥了重要作用。
　　CNN可以用BP算法进行训练，但是在训练中，无论是卷积层还是采样层，其每组神经元（即上图中的每一个“平面”）都是用相同的连接权，
   从而大幅减少了需要训练的参数数目。
 
 
 
 
 
 
 
 
 
