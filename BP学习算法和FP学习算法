神经网络主要是由三个部分组成的,分别是:1) 网络架构 2) 激活函数  3) 找出最优权重值的参数学习算法.

## BP神经网络是这样一种神经网络模型，它是由一个输入层、一个输出层和一个或多个隐层构成，它的激活函数采用sigmoid函数，采用BP算法训练的多层前馈神经网络。

## BP算法：按照误差逆向传播算法训练的多层前馈神经网络
##    它的基本思想：学习过程由信号的正向传播(求损失)与误差的反向传播(误差回传)两个过程组成

BP网络训练过程（学习过程）就是权值调整过程 

BP算法全称叫作误差反向传播(error Back Propagation，或者也叫作误差逆传播)算法。其算法基本思想为：
在前馈网络中，输入信号经输入层输入，通过隐层计算由输出层输出，输出值与标记值比较，若有误差，将误差反向由输出层向输入层传播，
在这个过程中，利用梯度下降算法对神经元权值进行调整。

BP算法中核心的数学工具就是微积分的链式求导法则。


根据BP算法的基本思想,可以得到BP算法的一般过程:

    1) 正向传播FP(求损失).在这个过程中,我们根据输入的样本,给定的初始化权重值W和偏置项的值b, 计算最终输出值以及输出值与实际值之间的损失值.如果损失值不在给定的范围内则进行反向传播的过程; 否则停止W,b的更新.

    2) 反向传播BP(回传误差).将输出以某种形式通过隐层向输入层逐层反传,并将误差分摊给各层的所有单元，从而获得各层单元的误差信号,此误差信号即作为修正各单元权值的依据。

由于BP算法是通过传递误差值δ进行更新求解权重值W和偏置项的值b, 所以BP算法也常常被叫做δ算法.


## BP算法的梯度消失问题：
 梯度消失问题在神经网络层数相对较多的时会遇到，，梯度消失原因是链式求导，导致梯度逐层递减，我们BP第一节推倒公式时就是通过链式求导把各层连接起来的，
 但是因为激活函数是sigmod函数，取值在1和-1之间，因此每次求导都会比原来小，当层次较多时，就会导致求导结果也就是梯度接近于0。

## 除此之外，神经网络中有一个学习率（l）的概念，通常取0和1之间的值，并有助于找到全局最小赋活函数一般使用simoid函数（或者logistic函数）
https://www.cnblogs.com/mengfanrong/p/3761796.html

算法基本流程就是：

1、初始化网络权值和神经元的阈值（最简单的办法就是随机初始化）

2、前向传播：依照公式一层一层的计算隐层神经元和输出层神经元的输入和输出。

3、后向传播：依据公式修正权值和阈值

直到满足终止条件。


---------------------------------------------------------------------------------------------------------------------
https://blog.csdn.net/zaishuiyifangxym/article/details/97782705


###  FP-Growth算法，它采取如下分治策略：将提供频繁项集的数据库压缩到一棵频繁模式树(FP-tree)，但仍保留项集关联信息
FP-tree是一种特殊的前缀树，由频繁项头表


### 从FP树中挖掘频繁项集的步骤：
从FP树中获得条件模式基；
利用条件模式基，构建一个条件FP树；
迭代：重复上述两个步骤，知道树包含一个元素项为止。

条件模式基(conditional pattern base)就是以所查找的元素项为结尾的路径集合。


