神经网络主要是由三个部分组成的,分别是:1) 网络架构 2) 激活函数  3) 找出最优权重值的参数学习算法.

## BP神经网络是这样一种神经网络模型，它是由一个输入层、一个输出层和一个或多个隐层构成，它的激活函数采用sigmoid函数，采用BP算法训练的多层前馈神经网络。

## BP算法：按照误差逆向传播算法训练的多层前馈神经网络
##    它的基本思想：学习过程由信号的正向传播(求损失)与误差的反向传播(误差回传)两个过程组成

BP网络训练过程（学习过程）就是权值调整过程 

BP算法全称叫作误差反向传播(error Back Propagation，或者也叫作误差逆传播)算法。其算法基本思想为：
在前馈网络中，输入信号经输入层输入，通过隐层计算由输出层输出，输出值与标记值比较，若有误差，将误差反向由输出层向输入层传播，
在这个过程中，利用梯度下降算法对神经元权值进行调整。

BP算法中核心的数学工具就是微积分的链式求导法则。


根据BP算法的基本思想,可以得到BP算法的一般过程:

    1) 正向传播FP(求损失).在这个过程中,我们根据输入的样本,给定的初始化权重值W和偏置项的值b, 计算最终输出值以及输出值与实际值之间的损失值.如果损失值不在给定的范围内则进行反向传播的过程; 否则停止W,b的更新.

    2) 反向传播BP(回传误差).将输出以某种形式通过隐层向输入层逐层反传,并将误差分摊给各层的所有单元，从而获得各层单元的误差信号,此误差信号即作为修正各单元权值的依据。

由于BP算法是通过传递误差值δ进行更新求解权重值W和偏置项的值b, 所以BP算法也常常被叫做δ算法.


## BP算法的梯度消失问题：
 梯度消失问题在神经网络层数相对较多的时会遇到，，梯度消失原因是链式求导，导致梯度逐层递减，我们BP第一节推倒公式时就是通过链式求导把各层连接起来的，
 但是因为激活函数是sigmod函数，取值在1和-1之间，因此每次求导都会比原来小，当层次较多时，就会导致求导结果也就是梯度接近于0。

## 除此之外，神经网络中有一个学习率（l）的概念，通常取0和1之间的值，并有助于找到全局最小赋活函数一般使用simoid函数（或者logistic函数）
https://www.cnblogs.com/mengfanrong/p/3761796.html

算法基本流程就是：

1、初始化网络权值和神经元的阈值（最简单的办法就是随机初始化）

2、前向传播：依照公式一层一层的计算隐层神经元和输出层神经元的输入和输出。

3、后向传播：依据公式修正权值和阈值

直到满足终止条件。


---------------------------------------------------------------------------------------------------------------------
https://blog.csdn.net/zaishuiyifangxym/article/details/97782705


###  FP-Growth算法，它采取如下分治策略：将提供频繁项集的数据库压缩到一棵频繁模式树(FP-tree)，但仍保留项集关联信息
FP-tree是一种特殊的前缀树，由频繁项头表和项前缀树构成。FP-Growth算法基于以上的结构加快整个挖掘过程。

FP-growth算法主要有两个步骤：
构建FP树
从FP树中挖掘频繁项集

###  FP树的特点

一个元素项可以在一棵FP树中出现多次;
项集以路径+频率的方式存储在FP树中;
树节点中的频率表示的是集合中单个元素在其序列中出现的次数;
一条路径表示的是一个事务，如果事务完全一致，则路径可以重合;
相似项之间的链接即为节点链接(link)，主要是用来快速发现相似项的位置.

######  FP树的构建的步骤
统计原始事务集中各元素项出现的频率
支持度过滤   ----把不满足最小支持度的元素项给剔除
排序       ----根据频率大小，将支持度过滤后的元素项(即 频繁元素项)进行排序，并根据排序后的元素项，将原始事务集也进行排序。（这里需要注意的是：事务集在重排序的时候，我们进行了两步操作：第一步：按照频率大小对元素项进行排序；第二步：对于相同频率的元素项，则对关键字进行降序排列(顺序会影响FP的结构)。）
构建FP树   -----首先从空集(∅)开始，向其中不断添加频繁项集。如果树中已存在现有元素，则增加现有元素的值。如果现有元素不存在，则向树中添加一个分支

构建的树和原始的FP树不同的原因 ： 对项的关键字排序将会影响FP树的结构。进而，树的结构也将影响后续发现频繁项的结果。

## 


